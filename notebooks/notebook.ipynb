{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import pymongo\n",
    "import json\n",
    "import pymongo\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en import English\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "nlp = English()\n",
    "stop = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "\n",
    "wiki_db = client.wikipedia\n",
    "\n",
    "wiki_col = wiki_db.my_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions to get data from Wiki API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def category_request(category):\n",
    "    \"\"\"\n",
    "    Scrape a category page from Wikipedia API.\n",
    "    \n",
    "    Params:\n",
    "    --------\n",
    "    category: str\n",
    "        The name of the category to be scraped.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Pandas DataFrame containing categories \n",
    "        \n",
    "    \"\"\"\n",
    "    my_params = {\n",
    "        'action':'query',\n",
    "        'format':'json',\n",
    "        'list':'categorymembers',\n",
    "        'cmtitle': 'Category:{}'.format(category),\n",
    "        'cmlimit': 'max'\n",
    "        }\n",
    "    page = requests.get('http://en.wikipedia.org/w/api.php', params=my_params)\n",
    "    return pd.DataFrame(page.json()['query']['categorymembers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_content(title):\n",
    "    \"\"\"\n",
    "    Scrape a page from Wikipedia API to get the content.\n",
    "    \n",
    "    Params:\n",
    "    --------\n",
    "    title: str\n",
    "        The name of the page to be scraped.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List of the contents of the page\n",
    "        \n",
    "    \"\"\"\n",
    "    my_params = {\n",
    "        'action':'query',\n",
    "        'format':'json',\n",
    "        'titles': title,\n",
    "        'prop': 'revisions',\n",
    "        'rvprop': 'content'\n",
    "    }\n",
    "    content = requests.get('http://en.wikipedia.org/w/api.php', params=my_params)\n",
    "    return list(content.json()['query']['pages'].values())[0]['revisions'][0]['*']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cats_and_pages(category):\n",
    "    \"\"\"\n",
    "    Returns the pages and subcategories of a category\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    category : str\n",
    "        Name of a category\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    children: list \n",
    "        list of sub categories \n",
    "    pages: list\n",
    "        list of pages on the category\n",
    "    \n",
    "    page_id = list of page_ids for each page\n",
    "        \n",
    "    \"\"\"\n",
    "    cats = pd.DataFrame(category_request(category))\n",
    "    cats['title'] = cats.title.astype(str) \n",
    "    #returns a boolean mask of all titles with 'category' in the str\n",
    "    subs_mask = cats['title'].str.contains('Category:')\n",
    "    \n",
    "    #creates list of new sub catagories\n",
    "    children = list(cats['title'][subs_mask].str.replace('Category:', \"\"))\n",
    "    pages = list(cats['title'][~cats.title.str.contains('Category:')])\n",
    "    page_id = list(cats['pageid'][~cats.title.str.contains('Category:')])\n",
    "    return page_id, pages, children\n",
    "\n",
    "#sub_categories, pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    text = re.sub('&#39;','',text).lower()\n",
    "    text = re.sub('<br />','',text)\n",
    "    text = re.sub('<.*>.*</.*>','', text)\n",
    "    text = re.sub('[\\d]','',text)\n",
    "    text = re.sub('[^a-z ]',' ',text)\n",
    "    text = re.sub(u'<.*>','',text)\n",
    "    text = re.sub(u'[^a-z\\s]',' ',text)\n",
    "    text = re.sub(\"\\\\s+\", \" \", text)\n",
    "    text = nlp(text)\n",
    "    text = [str(i.lemma_) for i in text if str(i.orth_) not in stop]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def wiki_traverse(main_cat, category, max_depth=-1):\n",
    "    \"\"\" \n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    category : str\n",
    "        Name of a category\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Does not return anything, function automatically feeds dictionaries of category, articles/\n",
    "    and content into Mongo database. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if max_depth != 0:\n",
    "\n",
    "        page_id, pages, children = get_cats_and_pages(category)\n",
    "        \n",
    "        for index, article in enumerate(pages):    \n",
    "            article_dict = {}\n",
    "            article_dict['main_cat'] = main_cat\n",
    "            article_dict['sub_cat'] = category\n",
    "            article_dict['article'] = article\n",
    "            article_dict['page_id'] = str(page_id[index])  \n",
    "            article_dict['content'] = cleaner(get_content(article))\n",
    "            \n",
    "            #this line adds each article onto mongo database as each article is being called \n",
    "            wiki_col.insert_one(article_dict)\n",
    "            \n",
    "        for child in children:\n",
    "            wiki_traverse(main_cat, child, max_depth-1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose to only collect a max depth of 3 for the categories because subcategories beyond depth 4 did not seem relevant to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_traverse('Business software', 'Business software', max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_traverse('Machine learning', 'Machine learning', max_depth=3)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5785"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_col.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
