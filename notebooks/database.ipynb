{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amit/Practice/test\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import standard modules\n",
    "%run lib/__init__.py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IP='localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(IP, 27017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check existing databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['local', 'myWiki', 'wikipedia']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a reference to a new database for wikipedia information ( lets refer to database refs as _dbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myWiki_dbr=client.myWiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a reference to a test collection  ( lets refer to collection refs with _cllr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_cllr=myWiki_dbr.test_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a test record to create db and collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x7f6f7a80ad80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_record={ 'message' : 'Test db creation'}\n",
    "test_cllr.insert_one(test_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maintenance of DB content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dbs and collections set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['local', 'myWiki', 'wikipedia']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.database_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pagetestload_collection',\n",
       " 'test_collection',\n",
       " 'loads_collection',\n",
       " 'page_collection',\n",
       " 'pagetest_collection',\n",
       " 'category_collection',\n",
       " 'pageload_collection']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myWiki_dbr.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check documents inserted\n",
    "- Assign a query to a cursor\n",
    "- run query by calling cursor\n",
    "- Use cursor to iterate through query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('5adde80f023fe31e0ff7d85d'), 'message': 'Test db creation'},\n",
       " {'_id': ObjectId('5addfe4d023fe324ebb363ed'), 'message': 'Test db creation'},\n",
       " {'_id': ObjectId('5ae630a3023fe316b0ce5bcd'), 'message': 'Test db creation'},\n",
       " {'_id': ObjectId('5ae75de0023fe3241a990db0'), 'message': 'Test db creation'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor=test_cllr.find()\n",
    "cursor\n",
    "docs=list(cursor)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename a collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check content of collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor=test_cllr.find().count()\n",
    "cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loaded</th>\n",
       "      <th>master_cat</th>\n",
       "      <th>sub_cats_added</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Apr 29 21:54:10 2018</td>\n",
       "      <td>Category:Classification algorithms</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sun Apr 29 22:05:48 2018</td>\n",
       "      <td>Category:business software</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Apr 30 13:57:06 2018</td>\n",
       "      <td>Category:Machine learning</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon Apr 30 14:29:48 2018</td>\n",
       "      <td>Category:neymar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Apr 30 14:30:13 2018</td>\n",
       "      <td>Category:quantum</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mon Apr 30 14:31:57 2018</td>\n",
       "      <td>Category:business</td>\n",
       "      <td>2215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     loaded                          master_cat  \\\n",
       "0  Sun Apr 29 21:54:10 2018  Category:Classification algorithms   \n",
       "1  Sun Apr 29 22:05:48 2018          Category:business software   \n",
       "2  Mon Apr 30 13:57:06 2018           Category:Machine learning   \n",
       "3  Mon Apr 30 14:29:48 2018                     Category:neymar   \n",
       "4  Mon Apr 30 14:30:13 2018                    Category:quantum   \n",
       "5  Mon Apr 30 14:31:57 2018                   Category:business   \n",
       "\n",
       "   sub_cats_added  \n",
       "0               5  \n",
       "1             226  \n",
       "2              45  \n",
       "3               0  \n",
       "4               0  \n",
       "5            2215  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loads_cllr=myWiki_dbr.loads_collection\n",
    "cursor=loads_cllr.find()\n",
    "cdocs=pd.DataFrame(list(cursor))\n",
    "try:\n",
    "    cdocs=cdocs.drop('_id',1)\n",
    "except:\n",
    "    cdocs\n",
    "display(cdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2491"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category_cllr=myWiki_dbr.category_collection\n",
    "cursor=category_cllr.find().count()\n",
    "display(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pagetest_cllr=myWiki_dbr.pagetest_collection\n",
    "cursor=pagetest_cllr.find().count()\n",
    "display(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28522"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "page_cllr=myWiki_dbr.page_collection\n",
    "cursor=page_cllr.find().count()\n",
    "display(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>loaded</th>\n",
       "      <th>master_cat</th>\n",
       "      <th>pages_added</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5ae631a1023fe316d2ab7e8a</td>\n",
       "      <td>Sun Apr 29 21:57:05 2018</td>\n",
       "      <td>Category:Classification algorithms</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                    loaded  \\\n",
       "0  5ae631a1023fe316d2ab7e8a  Sun Apr 29 21:57:05 2018   \n",
       "\n",
       "                           master_cat  pages_added  \n",
       "0  Category:Classification algorithms          262  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagetestload_cllr=myWiki_dbr.pagetestload_collection\n",
    "cursor=pagetestload_cllr.find()\n",
    "doc=list(cursor)\n",
    "pd.DataFrame(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loaded</th>\n",
       "      <th>master_cat</th>\n",
       "      <th>pages_added</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Apr 29 23:01:39 2018</td>\n",
       "      <td>Category:business software</td>\n",
       "      <td>5124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Apr 30 14:09:46 2018</td>\n",
       "      <td>Category:Machine learning</td>\n",
       "      <td>1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Apr 30 14:29:59 2018</td>\n",
       "      <td>Category:neymar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     loaded                  master_cat  pages_added\n",
       "0  Sun Apr 29 23:01:39 2018  Category:business software         5124\n",
       "1  Mon Apr 30 14:09:46 2018   Category:Machine learning         1135\n",
       "2  Mon Apr 30 14:29:59 2018             Category:neymar            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pageload_cllr=myWiki_dbr.pageload_collection\n",
    "cursor=pageload_cllr.find()\n",
    "pdocs=pd.DataFrame(list(cursor))\n",
    "try: \n",
    "    pdocs=pdocs.drop('_id',1)\n",
    "except:\n",
    "    pdocs\n",
    "display(pdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_wiki_page(page):\n",
    "    '''\n",
    "    pull page extract and category list for a specified page using \n",
    "    wikipedia API\n",
    "    '''\n",
    "    \n",
    "    #pull page extract\n",
    "    params_p = {\"action\": \"query\",\n",
    "            \"titles\": page,\n",
    "            \"prop\": 'extracts' ,\n",
    "            \"format\": 'json' ,\n",
    "        }\n",
    "            \n",
    "    response_p = requests.get(\"https://en.wikipedia.org/w/api.php\", params = params_p)\n",
    "    data = response_p.json()\n",
    "    \n",
    "    #pull page categories\n",
    "    params_c = {\"action\": \"query\",\n",
    "            \"titles\": page,\n",
    "            \"prop\": 'categories' ,\n",
    "            \"format\": 'json' ,\n",
    "            }\n",
    "            \n",
    "    response_c = requests.get(\"https://en.wikipedia.org/w/api.php\", params = params_c)\n",
    "    cats = response_c.json()\n",
    "            \n",
    "    return data , cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import nltk  # natural language toolkit\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/amit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en')\n",
    "\n",
    "#download stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#create list of english stop words from nltk\n",
    "nltk_stop=stopwords.words('english')\n",
    "#add addtional items to list of stopwords\n",
    "nltk_stop.append('displaystyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    ''' Clean text data, apply spacy lemmatization and nltk stop words'''\n",
    "    text = re.sub('<* />','',text)\n",
    "    text = re.sub('<.*>.*</.*>','', text)\n",
    "    text = re.sub('[\\d]',' ',text)\n",
    "    text = re.sub('{*}',' ',text)\n",
    "    text = re.sub('[\\n]',' ',text)\n",
    "    text = re.sub('[^a-zA-Z ]',' ',text)\n",
    "    text = ' '.join(i.lemma_ for i in nlp(text)\n",
    "                    if i.orth_ not in nltk_stop)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_articles( pageid, page):\n",
    "        '''\n",
    "        read page extract and category list from wikipedi \n",
    "        API query and add to page dict\n",
    "        '''\n",
    "        data, cats=pull_wiki_page(page)\n",
    "        \n",
    "        # first data clean - extra from json query and parse html\n",
    "        article=data['query']['pages'][pageid]['extract']\n",
    "        soup = BeautifulSoup(article, 'html.parser')\n",
    "        extract=soup.get_text()\n",
    "        \n",
    "        # 2nd data clean including lemmatization and stop words\n",
    "        \n",
    "        extract_clean=cleaner(extract)\n",
    "\n",
    "        return article , soup, extract , extract_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article , soup, extract , extract_clean = read_articles('3771060', 'Accuracy paradox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The accuracy paradox for predictive analytics states that predictive models with a given level of accuracy may have greater predictive power than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as precision and recall.\\nAccuracy is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. Accuracy measures the ratio of correct predictions to the total number of cases evaluated. It may seem obvious that the ratio of correct predictions to cases should be a key metric. A predictive model may have high accuracy, but be useless.\\nIn an example predictive model for an insurance fraud application, all cases that are predicted as high-risk by the model will be investigated. To evaluate the performance of the model, the insurance company has created a sample data set of 10,000 claims. All 10,000 cases in the validation sample have been carefully checked and it is known which cases are fraudulent. A table of confusion assists analyzing the quality of the model. The definition of accuracy, the table of confusion for model M1Fraud, and the calculation of accuracy for model M1Fraud is shown below.\\n\\n\\n\\n\\n\\nA\\n\\n(\\nM\\n)\\n=\\n\\n\\n\\nT\\nN\\n+\\nT\\nP\\n\\n\\nT\\nN\\n+\\nF\\nP\\n+\\nF\\nN\\n+\\nT\\nP\\n\\n\\n\\n\\n\\n{\\\\displaystyle \\\\mathrm {A} (M)={\\\\frac {TN+TP}{TN+FP+FN+TP}}}\\n where\\nTN is the number of true negative cases\\nFP is the number of false positive cases\\nFN is the number of false negative cases\\nTP is the number of true positive cases\\nFormula 1: Definition of Accuracy\\nTable 1: Table of Confusion for Fraud Model M1Fraud.\\n\\n\\n\\n\\n\\nA\\n\\n(\\nM\\n)\\n=\\n\\n\\n\\n9\\n,\\n700\\n+\\n100\\n\\n\\n9\\n,\\n700\\n+\\n150\\n+\\n50\\n+\\n100\\n\\n\\n\\n=\\n98.0\\n%\\n\\n\\n{\\\\displaystyle \\\\mathrm {A} (M)={\\\\frac {9,700+100}{9,700+150+50+100}}=98.0\\\\%}\\n\\nFormula 2: Accuracy for model M1Fraud\\nWith an accuracy of 98.0% model M1Fraud appears to perform fairly well. The paradox lies in the fact that accuracy can be easily improved to 98.5% by always predicting \"no fraud\". The table of confusion and the accuracy for this trivial “always predict negative” model M2Fraud and the accuracy of this model are shown below.\\nTable 2: Table of Confusion for Fraud Model M2Fraud.\\n\\n\\n\\n\\n\\nA\\n\\n(\\nM\\n)\\n=\\n\\n\\n\\n9\\n,\\n850\\n+\\n0\\n\\n\\n9\\n,\\n850\\n+\\n150\\n+\\n0\\n+\\n0\\n\\n\\n\\n=\\n98.5\\n%\\n\\n\\n{\\\\displaystyle \\\\mathrm {A} (M)={\\\\frac {9,850+0}{9,850+150+0+0}}=98.5\\\\%}\\n\\nFormula 3: Accuracy for model M2Fraud\\nModel M2Fraudreduces the rate of inaccurate predictions from 2% to 1.5%. This is an apparent improvement of 25%. The new model M2Fraud shows fewer incorrect predictions and markedly improved accuracy, as compared to the original model M1Fraud, but is obviously useless.\\nThe alternative model M2Fraud does not offer any value to the company for preventing fraud. The less accurate model is more useful than the more accurate model.\\nCaution is advised when using accuracy in the evaluation of predictive models; it is appropriate only if the cost of a false positive (false alarm) is equal to the cost of a false negative (missed prediction). Otherwise, a more appropriate loss function should be determined.\\nSee also\\nReceiver operating characteristic for other measures of how good model predictions are.\\nReferences\\nGeneral references'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'lemma_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d2fa11d5a89e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'lemma_'"
     ]
    }
   ],
   "source": [
    "dir(doc)\n",
    "l=[]\n",
    "for i in doc:\n",
    "    if i.lemma_ not in nltk_stop:\n",
    "        l.append(i.lemma_)\n",
    "\n",
    "display(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The accuracy paradox for predictive analytics states that predictive models with a given level of accuracy may have greater predictive power than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as precision and recall.\n",
       "Accuracy is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. Accuracy measures the ratio of correct predictions to the total number of cases evaluated. It may seem obvious that the ratio of correct predictions to cases should be a key metric. A predictive model may have high accuracy, but be useless.\n",
       "In an example predictive model for an insurance fraud application, all cases that are predicted as high-risk by the model will be investigated. To evaluate the performance of the model, the insurance company has created a sample data set of 10,000 claims. All 10,000 cases in the validation sample have been carefully checked and it is known which cases are fraudulent. A table of confusion assists analyzing the quality of the model. The definition of accuracy, the table of confusion for model M1Fraud, and the calculation of accuracy for model M1Fraud is shown below.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "A\n",
       "\n",
       "(\n",
       "M\n",
       ")\n",
       "=\n",
       "\n",
       "\n",
       "\n",
       "T\n",
       "N\n",
       "+\n",
       "T\n",
       "P\n",
       "\n",
       "\n",
       "T\n",
       "N\n",
       "+\n",
       "F\n",
       "P\n",
       "+\n",
       "F\n",
       "N\n",
       "+\n",
       "T\n",
       "P\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "{\\displaystyle \\mathrm {A} (M)={\\frac {TN+TP}{TN+FP+FN+TP}}}\n",
       " where\n",
       "TN is the number of true negative cases\n",
       "FP is the number of false positive cases\n",
       "FN is the number of false negative cases\n",
       "TP is the number of true positive cases\n",
       "Formula 1: Definition of Accuracy\n",
       "Table 1: Table of Confusion for Fraud Model M1Fraud.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "A\n",
       "\n",
       "(\n",
       "M\n",
       ")\n",
       "=\n",
       "\n",
       "\n",
       "\n",
       "9\n",
       ",\n",
       "700\n",
       "+\n",
       "100\n",
       "\n",
       "\n",
       "9\n",
       ",\n",
       "700\n",
       "+\n",
       "150\n",
       "+\n",
       "50\n",
       "+\n",
       "100\n",
       "\n",
       "\n",
       "\n",
       "=\n",
       "98.0\n",
       "%\n",
       "\n",
       "\n",
       "{\\displaystyle \\mathrm {A} (M)={\\frac {9,700+100}{9,700+150+50+100}}=98.0\\%}\n",
       "\n",
       "Formula 2: Accuracy for model M1Fraud\n",
       "With an accuracy of 98.0% model M1Fraud appears to perform fairly well. The paradox lies in the fact that accuracy can be easily improved to 98.5% by always predicting \"no fraud\". The table of confusion and the accuracy for this trivial “always predict negative” model M2Fraud and the accuracy of this model are shown below.\n",
       "Table 2: Table of Confusion for Fraud Model M2Fraud.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "A\n",
       "\n",
       "(\n",
       "M\n",
       ")\n",
       "=\n",
       "\n",
       "\n",
       "\n",
       "9\n",
       ",\n",
       "850\n",
       "+\n",
       "0\n",
       "\n",
       "\n",
       "9\n",
       ",\n",
       "850\n",
       "+\n",
       "150\n",
       "+\n",
       "0\n",
       "+\n",
       "0\n",
       "\n",
       "\n",
       "\n",
       "=\n",
       "98.5\n",
       "%\n",
       "\n",
       "\n",
       "{\\displaystyle \\mathrm {A} (M)={\\frac {9,850+0}{9,850+150+0+0}}=98.5\\%}\n",
       "\n",
       "Formula 3: Accuracy for model M2Fraud\n",
       "Model M2Fraudreduces the rate of inaccurate predictions from 2% to 1.5%. This is an apparent improvement of 25%. The new model M2Fraud shows fewer incorrect predictions and markedly improved accuracy, as compared to the original model M1Fraud, but is obviously useless.\n",
       "The alternative model M2Fraud does not offer any value to the company for preventing fraud. The less accurate model is more useful than the more accurate model.\n",
       "Caution is advised when using accuracy in the evaluation of predictive models; it is appropriate only if the cost of a false positive (false alarm) is equal to the cost of a false negative (missed prediction). Otherwise, a more appropriate loss function should be determined.\n",
       "See also\n",
       "Receiver operating characteristic for other measures of how good model predictions are.\n",
       "References\n",
       "General references"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2066"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'the accuracy paradox predictive analytic state predictive model give level accuracy may great predictive power model high accuracy -PRON- may good avoid accuracy metric favor metric precision recall accuracy often starting point analyze quality predictive model well obvious criterion prediction accuracy measure ratio correct prediction total number case evaluate -PRON- may seem obvious ratio correct prediction case key metric a predictive model may high accuracy useless in example predictive model insurance fraud application case predict high risk model investigate to evaluate performance model insurance company create sample datum set claim all case validation sample carefully check know case fraudulent a table confusion assist analyze quality model the definition accuracy table confusion model m fraud calculation accuracy model m fraud show a m t n t p t n f p f n t p mathrm a m frac tn tp tn fp fn tp tn number true negative case fp number false positive case fn number false negative case tp number true positive case formula definition accuracy table table confusion fraud model m fraud a m mathrm a m frac formula accuracy model m fraud with accuracy model m fraud appear perform fairly well the paradox lie fact accuracy easily improve always predict fraud the table confusion accuracy trivial always predict negative model m fraud accuracy model show table table confusion fraud model m fraud a m mathrm a m frac formula accuracy model m fraud model m fraudreduces rate inaccurate prediction this apparent improvement the new model m fraud show few incorrect prediction markedly improved accuracy compare original model m fraud obviously useless the alternative model m fraud offer value company prevent fraud the less accurate model useful accurate model caution advise use accuracy evaluation predictive model appropriate cost false positive false alarm equal cost false negative miss prediction otherwise appropriate loss function determine see also receiver operating characteristic measure good model prediction references general reference'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at impact of cleaning\n",
    "display(len(extract_clean))\n",
    "extract_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner_v2(text):\n",
    "    ''' Clean text data, apply spacy lemmatization and nltk stop words'''\n",
    "    text = re.sub('{.*}',' ',text)\n",
    "    #text = re.sub('<* />','',text)   # not neeeded as already removed by beautiful soup\n",
    "    #text = re.sub('<.*>.*</.*>','', text) # not neeeded as already removed by beautiful soup\n",
    "    #text = re.sub('[\\d]',' ',text) # not needed as removed by last 3 steps\n",
    "    #text = re.sub('[\\n]',' ',text) # not needed as removed by last 3 steps\n",
    "    text = re.sub('[^a-zA-Z ]',' ',text) # remove numbers and characters not in latin alphabet \n",
    "    text = ' '.join(i.lemma_ for i in nlp(text)\n",
    "                    if i.lemma_ not in nltk_stop)\n",
    "    text = re.sub('-PRON-',' ',text)  # added by spacy lemmatization ?? - remove\n",
    "    text = ' '.join(i for i in text.split() if len(i)!=1)  # remove redundant spaces and individual letters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'accuracy paradox predictive analytic state predictive model give level accuracy may great predictive power model high accuracy may good avoid accuracy metric favor metric precision recall accuracy often starting point analyze quality predictive model well obvious criterion prediction accuracy measure ratio correct prediction total number case evaluate may seem obvious ratio correct prediction case key metric predictive model may high accuracy useless example predictive model insurance fraud application case predict high risk model investigate evaluate performance model insurance company create sample datum set claim case validation sample carefully check know case fraudulent table confusion assist analyze quality model definition accuracy table confusion model fraud calculation accuracy model fraud show tn number true negative case fp number false positive case fn number false negative case tp number true positive case formula definition accuracy table table confusion fraud model fraud formula accuracy model fraud accuracy model fraud appear perform fairly well paradox lie fact accuracy easily improve always predict fraud table confusion accuracy trivial always predict negative model fraud accuracy model show table table confusion fraud model fraud formula accuracy model fraud model fraudreduces rate inaccurate prediction apparent improvement new model fraud show incorrect prediction markedly improved accuracy compare original model fraud obviously useless alternative model fraud offer value company prevent fraud less accurate model useful accurate model caution advise use accuracy evaluation predictive model appropriate cost false positive false alarm equal cost false negative miss prediction otherwise appropriate loss function determine see also receiver operating characteristic measure good model prediction references general reference'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at impact of revised cleaner\n",
    "ec=cleaner_v2(extract)\n",
    "print(len(ec))\n",
    "ec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
